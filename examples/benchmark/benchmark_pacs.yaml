# test benchmark config.

test_domains:
  - sketch

output_dir: zoutput/benchmarks/pacs_benchmark
# number of hyperparameter samples per task.
# Thus, the total runs of each task are given
# by len(domains) * num_param_samples * num_seeds (see below)
num_param_samples: 8
sampling_seed: 0

startseed: 0
endseed: 5  # currently included



domainlab_args:
  # Domainlab arguments passed to each task.
  # task specific arguments take precedence.
  ## dataset
  tpath: examples/tasks/task_pacs_path_list.py
  dmem: False
  lr: 1e-5
  epos: 500
  es: 5
  bs: 64
  nname: alexnet
  san_check: True


Task1:  # name
  model: diva
  # set nname_dom = nname when changing nname
  nname_dom: alexnet

  hyperparameters:
    gamma_y:
      min: 1e4
      max: 2e5
      step: 100
      distribution: loguniform

    gamma_d:
      min: 1e4
      max: 1e6
      step: 10_000
      distribution: loguniform

    zx_dim:
      min: 0
      max: 96
      step: 32
      distribution: uniform

    zy_dim:
      min: 32
      max: 96
      step: 32
      distribution: uniform

    zd_dim:
      reference: zy_dim

Task2:  # name
  model: hduva

  nname_encoder_x2topic_h: alexnet
  nname_encoder_sandwich_x2h4zd: alexnet

  hyperparameters:
    # Same config as diva.
    gamma_y:
      min: 1e4
      max: 2e5
      step: 100
      distribution: loguniform

    zx_dim:
      min: 0
      max: 96
      step: 32
      distribution: uniform

    zy_dim:
      min: 32
      max: 96
      step: 32
      distribution: uniform

    zd_dim:
      reference: zy_dim

Task3:  # name
  model: erm
  trainer: matchdg

  hyperparameters:
    # Total number of epochs for contrastive loss
    epochs_ctr:
      min: 2
      max: 10
      step: 1
      distribution: uniform

    # factor to magnify cosine similarity
    tau:
      min: 0.01
      max: 1
      distribution: loguniform

    # Number of epochs before updating the match tensor
    epos_per_match_update:
      min: 1
      max: 20
      step: 1
      distribution: uniform

    # penalty weight for matching loss (Lambda in paper)
    gamma_reg:
      min: 0.01
      max: 10
      distribution: loguniform

Task4:  # name
  model: jigen
  grid_len: 3

  hyperparameters:
    # number of permutations
    nperm:
      distribution: categorical
      values:
        - 30
        - 31
        - 100
      datatype: int

    # probability of permutating the tiles of an image, pperm = 0 -> pure classification
    pperm:
      min: 0.1
      max: 0.5
      distribution: uniform

    gamma_reg:
      min: 0.01
      max: 10
      distribution: loguniform


Task5:  # name
  model: dann

  hyperparameters:
    gamma_reg:
      min: 0.01
      max: 10
      distribution: loguniform

Task6:  # name
  model: erm


Task7: # name
  model: erm_dial

  hyperparameters:
    dial_steps_perturb:
      min: 1
      max: 10
      step: 1
      distribution: uniform

    dial_noise_scale:
      min: 0.00001
      max: 0.1
      distribution: loguniform

    dial_epsilon:
      min: 0.001
      max: 0.01
      distribution: loguniform

    gamma_reg:
      min: 0.01
      max: 10
      distribution: loguniform
