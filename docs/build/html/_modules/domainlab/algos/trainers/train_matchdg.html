<!DOCTYPE html>

<html lang="en" data-content_root="../../../../">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <meta name="viewport" content="width=device-width,initial-scale=1">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <meta name="lang:clipboard.copy" content="Copy to clipboard">
  <meta name="lang:clipboard.copied" content="Copied to clipboard">
  <meta name="lang:search.language" content="en">
  <meta name="lang:search.pipeline.stopwords" content="True">
  <meta name="lang:search.pipeline.trimmer" content="True">
  <meta name="lang:search.result.none" content="No matching documents">
  <meta name="lang:search.result.one" content="1 matching document">
  <meta name="lang:search.result.other" content="# matching documents">
  <meta name="lang:search.tokenizer" content="[\s\-]+">

  
    <link href="https://fonts.gstatic.com/" rel="preconnect" crossorigin>
    <link href="https://fonts.googleapis.com/css?family=Roboto+Mono:400,500,700|Roboto:300,400,400i,700&display=fallback" rel="stylesheet">

    <style>
      body,
      input {
        font-family: "Roboto", "Helvetica Neue", Helvetica, Arial, sans-serif
      }

      code,
      kbd,
      pre {
        font-family: "Roboto Mono", "Courier New", Courier, monospace
      }
    </style>
  

  <link rel="stylesheet" href="../../../../_static/stylesheets/application.css"/>
  <link rel="stylesheet" href="../../../../_static/stylesheets/application-palette.css"/>
  <link rel="stylesheet" href="../../../../_static/stylesheets/application-fixes.css"/>
  
  <link rel="stylesheet" href="../../../../_static/fonts/material-icons.css"/>
  
  <meta name="theme-color" content="#2196f3">
  <script src="../../../../_static/javascripts/modernizr.js"></script>
  
  
    <link rel="apple-touch-icon" href="../../../../_static/images/apple-icon-152x152.png"/>
  
  
    <title>domainlab.algos.trainers.train_matchdg &#8212; domainlab  documentation</title>
    <link rel="stylesheet" type="text/css" href="../../../../_static/pygments.css?v=80d5e7a1" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/material.css?v=79c92029" />
    <script src="../../../../_static/documentation_options.js?v=5929fcd5"></script>
    <script src="../../../../_static/doctools.js?v=888ff710"></script>
    <script src="../../../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <link rel="index" title="Index" href="../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../search.html" />
  
    <link rel="apple-touch-icon" href="../../../../_static/images/apple-icon-152x152.png"/>
  
   

  </head>
  <body dir=ltr
        data-md-color-primary=teal data-md-color-accent=cyan>
  
  <svg class="md-svg">
    <defs data-children-count="0">
      
      <svg xmlns="http://www.w3.org/2000/svg" width="416" height="448" viewBox="0 0 416 448" id="__github"><path fill="currentColor" d="M160 304q0 10-3.125 20.5t-10.75 19T128 352t-18.125-8.5-10.75-19T96 304t3.125-20.5 10.75-19T128 256t18.125 8.5 10.75 19T160 304zm160 0q0 10-3.125 20.5t-10.75 19T288 352t-18.125-8.5-10.75-19T256 304t3.125-20.5 10.75-19T288 256t18.125 8.5 10.75 19T320 304zm40 0q0-30-17.25-51T296 232q-10.25 0-48.75 5.25Q229.5 240 208 240t-39.25-2.75Q130.75 232 120 232q-29.5 0-46.75 21T56 304q0 22 8 38.375t20.25 25.75 30.5 15 35 7.375 37.25 1.75h42q20.5 0 37.25-1.75t35-7.375 30.5-15 20.25-25.75T360 304zm56-44q0 51.75-15.25 82.75-9.5 19.25-26.375 33.25t-35.25 21.5-42.5 11.875-42.875 5.5T212 416q-19.5 0-35.5-.75t-36.875-3.125-38.125-7.5-34.25-12.875T37 371.5t-21.5-28.75Q0 312 0 260q0-59.25 34-99-6.75-20.5-6.75-42.5 0-29 12.75-54.5 27 0 47.5 9.875t47.25 30.875Q171.5 96 212 96q37 0 70 8 26.25-20.5 46.75-30.25T376 64q12.75 25.5 12.75 54.5 0 21.75-6.75 42 34 40 34 99.5z"/></svg>
      
    </defs>
  </svg>
  
  <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer">
  <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search">
  <label class="md-overlay" data-md-component="overlay" for="__drawer"></label>
  <a href="#_modules/domainlab/algos/trainers/train_matchdg" tabindex="1" class="md-skip"> Skip to content </a>
  <header class="md-header" data-md-component="header">
  <nav class="md-header-nav md-grid">
    <div class="md-flex navheader">
      <div class="md-flex__cell md-flex__cell--shrink">
        <a href="../../../../index.html" title="domainlab  documentation"
           class="md-header-nav__button md-logo">
          
            <i class="md-icon">school</i>
          
        </a>
      </div>
      <div class="md-flex__cell md-flex__cell--shrink">
        <label class="md-icon md-icon--menu md-header-nav__button" for="__drawer"></label>
      </div>
      <div class="md-flex__cell md-flex__cell--stretch">
        <div class="md-flex__ellipsis md-header-nav__title" data-md-component="title">
          <span class="md-header-nav__topic">DomainLab</span>
          <span class="md-header-nav__topic"> domainlab.algos.trainers.train_matchdg </span>
        </div>
      </div>
      <div class="md-flex__cell md-flex__cell--shrink">
        <label class="md-icon md-icon--search md-header-nav__button" for="__search"></label>
        
<div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" action="../../../../search.html" method="get" name="search">
      <input type="text" class="md-search__input" name="q" placeholder=""Search""
             autocapitalize="off" autocomplete="off" spellcheck="false"
             data-md-component="query" data-md-state="active">
      <label class="md-icon md-search__icon" for="__search"></label>
      <button type="reset" class="md-icon md-search__icon" data-md-component="reset" tabindex="-1">
        &#xE5CD;
      </button>
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="result">
          <div class="md-search-result__meta">
            Type to start searching
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>

      </div>
      
        <div class="md-flex__cell md-flex__cell--shrink">
          <div class="md-header-nav__source">
            <a href="https://github.com/marrlab/DomainLab" title="Go to repository" class="md-source" data-md-source="github">

    <div class="md-source__icon">
      <svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 24 24" width="28" height="28">
        <use xlink:href="#__github" width="24" height="24"></use>
      </svg>
    </div>
  
  <div class="md-source__repository">
    DomainLab
  </div>
</a>
          </div>
        </div>
      
      
    </div>
  </nav>
</header>

  
  <div class="md-container">
    
    
    
  <nav class="md-tabs" data-md-component="tabs">
    <div class="md-tabs__inner md-grid">
      <ul class="md-tabs__list">
            
            <li class="md-tabs__item"><a href="../../../../readme_link.html" class="md-tabs__link">Introduction</a></li>
            
            <li class="md-tabs__item"><a href="../../../../doc_tasks.html" class="md-tabs__link">Task Specification</a></li>
            
            <li class="md-tabs__item"><a href="../../../../doc_custom_nn.html" class="md-tabs__link">Specify neural network in commandline</a></li>
            
            <li class="md-tabs__item"><a href="../../../../doc_MNIST_classification.html" class="md-tabs__link">Examples with MNIST</a></li>
            
            <li class="md-tabs__item"><a href="../../../../doc_examples.html" class="md-tabs__link">More commandline examples</a></li>
            
            <li class="md-tabs__item"><a href="../../../../doc_benchmark.html" class="md-tabs__link">Benchmarks tutorial</a></li>
            
            <li class="md-tabs__item"><a href="../../../../doc_output.html" class="md-tabs__link">Output Structure</a></li>
            
            <li class="md-tabs__item"><a href="../../../../doc_extend_contribute.html" class="md-tabs__link">Specify custom model in commandline</a></li>
          <li class="md-tabs__item"><a href="../../../index.html" class="md-tabs__link">Module code</a></li>
          <li class="md-tabs__item"><a href="../../../domainlab.html" class="md-tabs__link">domainlab</a></li>
      </ul>
    </div>
  </nav>
    <main class="md-main">
      <div class="md-main__inner md-grid" data-md-component="container">
        
          <div class="md-sidebar md-sidebar--primary" data-md-component="navigation">
            <div class="md-sidebar__scrollwrap">
              <div class="md-sidebar__inner">
                <nav class="md-nav md-nav--primary" data-md-level="0">
  <label class="md-nav__title md-nav__title--site" for="__drawer">
    <a href="../../../../index.html" title="domainlab documentation" class="md-nav__button md-logo">
      
        <i class="md-icon">school</i>
      
    </a>
    <a href="../../../../index.html"
       title="domainlab documentation">DomainLab</a>
  </label>
    <div class="md-nav__source">
      <a href="https://github.com/marrlab/DomainLab" title="Go to repository" class="md-source" data-md-source="github">

    <div class="md-source__icon">
      <svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 24 24" width="28" height="28">
        <use xlink:href="#__github" width="24" height="24"></use>
      </svg>
    </div>
  
  <div class="md-source__repository">
    DomainLab
  </div>
</a>
    </div>
  
  

  
  <ul class="md-nav__list">
    <li class="md-nav__item">
    
      <span class="md-nav__link caption"><span class="caption-text">Contents:</span></span>
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="../../../../modules.html" class="md-nav__link">domainlab</a>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="../../../../docHDUVA.html" class="md-nav__link">Model HDUVA</a>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="../../../../doc_dann.html" class="md-nav__link">Model DANN</a>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="../../../../docJiGen.html" class="md-nav__link">Model JiGen</a>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="../../../../doc_diva.html" class="md-nav__link">Model DIVA</a>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="../../../../docMatchDG.html" class="md-nav__link">Trainer MatchDG</a>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="../../../../docDIAL.html" class="md-nav__link">Trainer DIAL</a>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="../../../../docFishr.html" class="md-nav__link">Trainer Fishr</a>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="../../../../doc_mldg.html" class="md-nav__link">Trainer MLDG</a>
      
    
    </li>
  </ul>
  

</nav>
              </div>
            </div>
          </div>
          <div class="md-sidebar md-sidebar--secondary" data-md-component="toc">
            <div class="md-sidebar__scrollwrap">
              <div class="md-sidebar__inner">
                
<nav class="md-nav md-nav--secondary">
  <ul class="md-nav__list" data-md-scrollfix="">
  </ul>
</nav>
              </div>
            </div>
          </div>
        
        <div class="md-content">
          <article class="md-content__inner md-typeset" role="main">
            
  <h1 id="modules-domainlab-algos-trainers-train-matchdg--page-root">Source code for domainlab.algos.trainers.train_matchdg</h1><div class="highlight"><pre>
<span></span>"""
trainer matchdg
"""
import torch

from domainlab import g_inst_component_loss_agg, g_list_loss_agg
from domainlab.algos.trainers.a_trainer import AbstractTrainer
from domainlab.algos.trainers.compos.matchdg_utils import \
get_base_domain_size4match_dg
from domainlab.algos.trainers.compos.matchdg_match import MatchPair
from domainlab.algos.trainers.compos.matchdg_utils import (dist_cosine_agg,
                                                  dist_pairwise_cosine)
from domainlab.utils.logger import Logger
from domainlab.tasks.utils_task_dset import DsetIndDecorator4XYD


<div class="viewcode-block" id="TrainerMatchDG">
<a class="viewcode-back" href="../../../../domainlab.algos.trainers.html#domainlab.algos.trainers.train_matchdg.TrainerMatchDG">[docs]</a>
class TrainerMatchDG(AbstractTrainer):
    """
    Contrastive Learning
    """
<div class="viewcode-block" id="TrainerMatchDG.dset_decoration_args_algo">
<a class="viewcode-back" href="../../../../domainlab.algos.trainers.html#domainlab.algos.trainers.train_matchdg.TrainerMatchDG.dset_decoration_args_algo">[docs]</a>
    def dset_decoration_args_algo(self, args, ddset):
        ddset = DsetIndDecorator4XYD(ddset)
        return ddset</div>


<div class="viewcode-block" id="TrainerMatchDG.init_business">
<a class="viewcode-back" href="../../../../domainlab.algos.trainers.html#domainlab.algos.trainers.train_matchdg.TrainerMatchDG.init_business">[docs]</a>
    def init_business(self, model, task, observer, device, aconf, flag_accept=True, flag_erm=False):
        """
        initialize member objects
        """
        super().init_business(model, task, observer, device, aconf, flag_accept)
        # use the same batch size for match tensor
        # so that order is kept!
        self.base_domain_size = get_base_domain_size4match_dg(self.task)
        self.epo_loss_tr = 0
        self.flag_erm = flag_erm
        self.lambda_ctr = self.aconf.gamma_reg
        self.mk_match_tensor(epoch=0)
        self.flag_match_tensor_sweep_over = False
        self.tuple_tensor_ref_domain2each_y = None
        self.tuple_tensor_refdomain2each = None</div>


<div class="viewcode-block" id="TrainerMatchDG.tr_epoch">
<a class="viewcode-back" href="../../../../domainlab.algos.trainers.html#domainlab.algos.trainers.train_matchdg.TrainerMatchDG.tr_epoch">[docs]</a>
    def tr_epoch(self, epoch):
        """
        # data in one batch comes from two sources: one part from loader,
        # the other part from match tensor
        """
        self.model.train()
        self.epo_loss_tr = 0
        logger = Logger.get_logger()
        # update match tensor
        if (epoch + 1) % self.aconf.epos_per_match_update == 0:
            self.mk_match_tensor(epoch)

        inds_shuffle = torch.randperm(self.tensor_ref_domain2each_domain_x.size(0))
        # NOTE: match tensor size: N(ref domain size) * #(train domains) * (image size: c*h*w)
        # self.tensor_ref_domain2each_domain_x[inds_shuffle]
        # shuffles the match tensor at the first dimension
        self.tuple_tensor_refdomain2each = torch.split(
            self.tensor_ref_domain2each_domain_x[inds_shuffle],
            self.aconf.bs, dim=0)
        # Splits the tensor into chunks.
        # Each chunk is a view of the original tensor of batch size self.aconf.bs
        # return is a tuple of the splited chunks
        self.tuple_tensor_ref_domain2each_y = torch.split(
            self.tensor_ref_domain2each_domain_y[inds_shuffle],
            self.aconf.bs, dim=0)
        logger.info(f"number of batches in match tensor: {len(self.tuple_tensor_refdomain2each)}")
        logger.info(f"single batch match tensor size: {self.tuple_tensor_refdomain2each[0].shape}")

        for batch_idx, (x_e, y_e, d_e, *others) in enumerate(self.loader_tr):
            # random loader with same batch size as the match tensor loader
            # the 4th output of self.loader is not used at all,
            # is only used for creating the match tensor
            self.tr_batch(epoch, batch_idx, x_e, y_e, d_e, others)
            if self.flag_match_tensor_sweep_over is True:
                logger.info("ref/base domain vs each domain match \
                            traversed one sweep, starting new epoch")
                self.flag_match_tensor_sweep_over = False
                break

        if epoch &lt; self.aconf.epochs_ctr:
            logger.info("\n\nPhase ctr-only continue\n\n")
            self.observer.reset()
            return False

        logger.info("\n\nPhase erm+ctr \n\n")
        self.flag_erm = True
        flag_stop = self.observer.update(epoch)  # notify observer
        return flag_stop</div>


<div class="viewcode-block" id="TrainerMatchDG.tr_batch">
<a class="viewcode-back" href="../../../../domainlab.algos.trainers.html#domainlab.algos.trainers.train_matchdg.TrainerMatchDG.tr_batch">[docs]</a>
    def tr_batch(self, epoch, batch_idx, x_e, y_e, d_e, others=None):
        """
        update network for each batch
        """
        self.optimizer.zero_grad()
        x_e = x_e.to(self.device)  # 64 * 1 * 224 * 224
        # y_e_scalar = torch.argmax(y_e, dim=1).to(self.device)
        y_e = y_e.to(self.device)
        # d_e = torch.argmax(d_e, dim=1).numpy()
        d_e = d_e.to(self.device)
        # for each batch, the list loss is re-initialized

        # CTR (contrastive) loss for CTR/ERM phase are different
        list_batch_loss_ctr = []
        # for a single batch,  loss need to be
        # aggregated across different combinations of domains.
        # Defining a leaf node can cause problem by loss_ctr += xxx,
        # a list with python built-in "sum" can aggregate
        # these losses within one batch

        if self.flag_erm:
            # decoratee can be both trainer or model
            list_loss_reg_rand, list_mu_reg = self.decoratee.cal_reg_loss(x_e, y_e, d_e, others)
            loss_reg = self.model.list_inner_product(list_loss_reg_rand, list_mu_reg)
            loss_task_rand = self.model.cal_task_loss(x_e, y_e)
            # loss_erm_rnd_loader, *_ = self.model.cal_loss(x_e, y_e, d_e, others)
            loss_erm_rnd_loader = loss_reg + loss_task_rand * self.model.multiplier4task_loss

        num_batches_match_tensor = len(self.tuple_tensor_refdomain2each)

        if batch_idx &gt;= num_batches_match_tensor:
            self.flag_match_tensor_sweep_over = True
            return

        curr_batch_size = self.tuple_tensor_refdomain2each[batch_idx].shape[0]

        batch_tensor_ref_domain2each = self.tuple_tensor_refdomain2each[batch_idx].to(self.device)
        # make order 5 tensor: (ref_domain, domain, channel, img_h, img_w)
        # with first dimension as batch size

        # clamp the first two dimensions so the model network could map image to feature
        batch_tensor_ref_domain2each = match_tensor_reshape(batch_tensor_ref_domain2each)
        # now batch_tensor_ref_domain2each first dim will not be batch_size!
        # batch_tensor_ref_domain2each.shape torch.Size([40, channel, 224, 224])

        batch_feat_ref_domain2each = self.model.extract_semantic_feat(
            batch_tensor_ref_domain2each)
        # batch_feat_ref_domain2each.shape torch.Size[40, 512]
        # torch.sum(torch.isnan(batch_tensor_ref_domain2each))
        # assert not torch.sum(torch.isnan(batch_feat_ref_domain2each))
        flag_isnan = torch.any(torch.isnan(batch_feat_ref_domain2each))
        if flag_isnan:
            logger = Logger.get_logger()
            logger.info(batch_tensor_ref_domain2each)
            raise RuntimeError("batch_feat_ref_domain2each NAN! is learning rate too big or"
                               "hyper-parameter tau not set appropriately?")

        # for contrastive training phase,
        # the last layer of the model is replaced with identity

        batch_ref_domain2each_y = self.tuple_tensor_ref_domain2each_y[batch_idx].to(self.device)
        batch_ref_domain2each_y = batch_ref_domain2each_y.view(
            batch_ref_domain2each_y.shape[0]*batch_ref_domain2each_y.shape[1])

        if self.flag_erm:
            # @FIXME: check if batch_ref_domain2each_y is
            # continuous number which means it is at its initial value,
            # not yet filled
            loss_erm_match_tensor, *_ = self.model.cal_task_loss(
                batch_tensor_ref_domain2each, batch_ref_domain2each_y.long())

        # Creating tensor of shape (domain size, total domains, feat size )
        # The match tensor's first two dimension
        # [(Ref domain size) * (# train domains)]
        # has been clamped together to get features extracted
        # through self.model

        # it has to be reshaped into the match tensor shape, the same
        # for the extracted feature here, it has to reshaped into
        # the shape of the match tensor
        # to make sure that the reshape only happens at the
        # first two dimension, the feature dim has to be kept intact
        dim_feat = batch_feat_ref_domain2each.shape[1]
        num_domain_tr = len(self.task.list_domain_tr)
        batch_feat_ref_domain2each = batch_feat_ref_domain2each.view(
            curr_batch_size, num_domain_tr, dim_feat)

        batch_ref_domain2each_y = batch_ref_domain2each_y.view(
            curr_batch_size, num_domain_tr)

        # The match tensor's first two dimension
        # [(Ref domain size) * (# train domains)] has been clamped
        # together to get features extracted through self.model
        batch_tensor_ref_domain2each = \
            batch_tensor_ref_domain2each.view(curr_batch_size,
                                              num_domain_tr,
                                              batch_tensor_ref_domain2each.shape[1],   # channel
                                              batch_tensor_ref_domain2each.shape[2],   # img_h
                                              batch_tensor_ref_domain2each.shape[3])   # img_w

        # Contrastive Loss: class \times domain \times domain
        counter_same_cls_diff_domain = 1
        logger = Logger.get_logger()
        for y_c in range(self.task.dim_y):

            subset_same_cls = (batch_ref_domain2each_y[:, 0] == y_c)
            subset_diff_cls = (batch_ref_domain2each_y[:, 0] != y_c)
            feat_same_cls = batch_feat_ref_domain2each[subset_same_cls]
            feat_diff_cls = batch_feat_ref_domain2each[subset_diff_cls]
            logger.debug(f'class {y_c} with same class and different class: ' +
                         f'{feat_same_cls.shape[0]} {feat_diff_cls.shape[0]}')

            if feat_same_cls.shape[0] == 0 or feat_diff_cls.shape[0] == 0:
                logger.debug(f"no instances of label {y_c}"
                             f"in the current batch, continue")
                continue

            if torch.sum(torch.isnan(feat_diff_cls)):
                raise RuntimeError('feat_diff_cls has nan entrie(s)')

            feat_diff_cls = feat_diff_cls.view(
                feat_diff_cls.shape[0]*feat_diff_cls.shape[1],
                feat_diff_cls.shape[2])

            for d_i in range(feat_same_cls.shape[1]):
                dist_diff_cls_same_domain = dist_pairwise_cosine(
                    feat_same_cls[:, d_i, :], feat_diff_cls[:, :])

                if torch.sum(torch.isnan(dist_diff_cls_same_domain)):
                    raise RuntimeError('dist_diff_cls_same_domain NAN')

                # iterate other domains
                for d_j in range(feat_same_cls.shape[1]):
                    if d_i &gt;= d_j:
                        continue
                    dist_same_cls_diff_domain = dist_cosine_agg(feat_same_cls[:, d_i, :],
                                                                feat_same_cls[:, d_j, :])

                    if torch.sum(torch.isnan(dist_same_cls_diff_domain)):
                        raise RuntimeError('dist_same_cls_diff_domain NAN')

                    # CTR (contrastive) loss is exclusive for
                    # CTR phase and ERM phase

                    if self.flag_erm:
                        list_batch_loss_ctr.append(torch.sum(dist_same_cls_diff_domain))
                    else:
                        i_dist_same_cls_diff_domain = 1.0 - dist_same_cls_diff_domain
                        i_dist_same_cls_diff_domain = \
                            i_dist_same_cls_diff_domain / self.aconf.tau
                        partition = torch.log(torch.exp(i_dist_same_cls_diff_domain) +
                                              dist_diff_cls_same_domain)
                        list_batch_loss_ctr.append(
                            -1 * torch.sum(i_dist_same_cls_diff_domain - partition))

                    counter_same_cls_diff_domain += dist_same_cls_diff_domain.shape[0]

        loss_ctr = g_list_loss_agg(list_batch_loss_ctr) / counter_same_cls_diff_domain

        if self.flag_erm:
            epos = self.aconf.epos
        else:
            epos = self.aconf.epochs_ctr
        percentage_finished_epochs = (epoch + 1)/(epos + 1)
        # loss aggregation is over different domain
        # combinations of the same batch
        # https://discuss.pytorch.org/t/leaf-variable-was-used-in-an-inplace-operation/308
        # Loosely, tensors you create directly are leaf variables.
        # Tensors that are the result of a differentiable operation are
        # not leaf variables

        if self.flag_erm:
            # extra loss of ERM phase: the ERM loss
            # (the CTR loss for the ctr phase and erm phase are different)
            # erm loss comes from two different data loaders,
            # one is rnd (random) data loader
            # the other one is the data loader from the match tensor
            loss_e = torch.tensor(0.0, requires_grad=True) + \
                    g_inst_component_loss_agg(loss_erm_rnd_loader) + \
                    g_inst_component_loss_agg(loss_erm_match_tensor) * self.model.multiplier4task_loss + \
                    self.lambda_ctr * percentage_finished_epochs * loss_ctr
        else:
            loss_e = torch.tensor(0.0, requires_grad=True) + \
                self.lambda_ctr * percentage_finished_epochs * loss_ctr
        # @FIXME: without torch.tensor(0.0), after a few epochs,
        # error "'float' object has no attribute 'backward'"

        loss_e.backward(retain_graph=False)
        self.optimizer.step()
        self.epo_loss_tr += loss_e.detach().item()

        torch.cuda.empty_cache()</div>


<div class="viewcode-block" id="TrainerMatchDG.mk_match_tensor">
<a class="viewcode-back" href="../../../../domainlab.algos.trainers.html#domainlab.algos.trainers.train_matchdg.TrainerMatchDG.mk_match_tensor">[docs]</a>
    def mk_match_tensor(self, epoch):
        """
        initialize or update match tensor
        """
        obj_match = MatchPair(self.task.dim_y,
                              self.task.isize.i_c,
                              self.task.isize.i_h,
                              self.task.isize.i_w,
                              self.aconf.bs,
                              virtual_ref_dset_size=self.base_domain_size,
                              num_domains_tr=len(self.task.list_domain_tr),
                              list_tr_domain_size=self.list_tr_domain_size)

        # @FIXME: what is the usefulness of (epoch &gt; 0) as argument
        self.tensor_ref_domain2each_domain_x, self.tensor_ref_domain2each_domain_y = \
        obj_match(
            self.device,
            self.task.loader_tr,
            self.model.extract_semantic_feat,
            (epoch &gt; 0))</div>


<div class="viewcode-block" id="TrainerMatchDG.before_tr">
<a class="viewcode-back" href="../../../../domainlab.algos.trainers.html#domainlab.algos.trainers.train_matchdg.TrainerMatchDG.before_tr">[docs]</a>
    def before_tr(self):
        """
        override abstract method
        """
        logger = Logger.get_logger()
        logger.info("\n\nPhase 1 start: contractive alignment without task loss: \n\n")</div>
</div>

        # phase 1: contrastive learning
        # different than phase 2, ctr_model has no classification loss


<div class="viewcode-block" id="match_tensor_reshape">
<a class="viewcode-back" href="../../../../domainlab.algos.trainers.html#domainlab.algos.trainers.train_matchdg.match_tensor_reshape">[docs]</a>
def match_tensor_reshape(batch_tensor_ref_domain2each): 
    """
    # original dimension is (ref_domain, domain, (channel, img_h, img_w))
    # use a function so it is easier to accomodate other data mode (not image)
    """
    batch_tensor_refdomain_other_domain_chw = batch_tensor_ref_domain2each.view(
        batch_tensor_ref_domain2each.shape[0]*batch_tensor_ref_domain2each.shape[1],
        batch_tensor_ref_domain2each.shape[2],   # channel
        batch_tensor_ref_domain2each.shape[3],   # img_h
        batch_tensor_ref_domain2each.shape[4])   # img_w
    return batch_tensor_refdomain_other_domain_chw</div>

</pre></div>

          </article>
        </div>
      </div>
    </main>
  </div>
  <footer class="md-footer">
    <div class="md-footer-nav">
      <nav class="md-footer-nav__inner md-grid">
          
          
        </a>
        
      </nav>
    </div>
    <div class="md-footer-meta md-typeset">
      <div class="md-footer-meta__inner md-grid">
        <div class="md-footer-copyright">
          <div class="md-footer-copyright__highlight">
              &#169; Copyright 2021-2024, Marr Lab..
              
          </div>
            Created using
            <a href="http://www.sphinx-doc.org/">Sphinx</a> 7.2.6.
             and
            <a href="https://github.com/bashtage/sphinx-material/">Material for
              Sphinx</a>
        </div>
      </div>
    </div>
  </footer>
  <script src="../../../../_static/javascripts/application.js"></script>
  <script>app.initialize({version: "1.0.4", url: {base: ".."}})</script>
  </body>
</html>